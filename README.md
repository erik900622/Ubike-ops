# Ubike 營運優化分析系統

一個基於時間序列數據分析的 Ubike 站點優化系統，透過即時資料收集與分析，協助營運團隊識別高風險站點並預測未來需求。

## User Story

> **作為** Ubike 營運管理者  
> **我希望** 能即時掌握全台北各站點的車輛供需狀況  
> **這樣** 我就能提前調度資源，減少「借不到車」和「還不了車」的情況，提升使用者體驗。

### 具體場景

- 當某站點可借車輛少於 3 台時，系統自動標記為「空車風險」，提醒補車
- 當某站點可還車位少於 3 個時，系統標記為「滿車風險」，提醒移車
- 透過歷史數據分析各時段、各區域的使用模式，優化調度策略

## 專案特色

### 📊 資料分析能力展示

1. **時間序列數據收集**
   - 每分鐘自動收集全台北 Ubike 站點資料
   - 建立時間序列資料庫，支援長期趨勢分析
   - 自動清理過期資料，維持資料庫效能

2. **風險站點識別**
   - 基於統計門檻值分析（可借車輛 ≤ 3、可還車位 ≤ 3）
   - 即時監控全台北數百個站點狀態
   - 產生每日營運報告

3. **需求預測模型**
   - 使用線性趨勢分析預測未來 30 分鐘車輛數
   - 平滑化局部變化，提升預測穩定性
   - 考量站點容量上下限，避免不合理預測

4. **視覺化分析**
   - 互動式地圖展示全台北站點即時狀態
   - 時段熱力圖分析各區域尖峰離峰時段
   - 流量趨勢圖呈現 24 小時需求變化

## 技術架構

```
專案結構
│
├── main.py              # 主程式：排程執行資料收集
├── requirements.txt     # Python 套件依賴
│
├── src/
│   ├── collector.py     # 資料收集：API 串接與錯誤處理
│   ├── database.py      # 資料庫：SQLite/PostgreSQL 支援
│   ├── analysis.py      # 分析模組：風險站點識別與統計
│   ├── prediction.py    # 預測模組：時間序列趨勢分析
│   ├── dashboard.py     # 分析儀表板：Streamlit 視覺化
│   └── config.py        # 設定檔：API、資料庫、風險閾值
│
└── scripts/             # 工具腳本：資料檢查與測試
```

### 資料流程

```
YouBike API
    ↓
資料收集 (每分鐘)
    ↓
時間序列資料庫
    ↓
分析 & 預測
    ↓
視覺化儀表板
```

## 核心功能

### 1. 自動化資料收集

- **資料來源**：新北市政府 YouBike 即時 API
- **收集頻率**：每分鐘一次
- **資料內容**：站點位置、可借車輛數、可還車位數、更新時間
- **容錯機制**：自動重試、網路異常處理

### 2. 風險站點分析

識別兩類高風險站點：

- **空車風險**：可借車輛 ≤ 3 台 → 需要補車
- **滿車風險**：可還車位 ≤ 3 個 → 需要移車

系統會產生每日報告，列出所有高風險站點及其當前狀態。

### 3. 需求預測

使用歷史數據預測未來 30 分鐘的車輛數量：

- 分析最近 30 筆資料點
- 計算平滑化的車輛變化趨勢
- 預測未來需求並標示風險等級

### 4. 視覺化儀表板

提供互動式分析介面（Streamlit）：

- **地圖視圖**：全台北站點分佈與即時狀態
- **熱力圖**：各區域 × 各時段的需求分析
- **趨勢圖**：24 小時流量變化
- **站點詳情**：單一站點的歷史數據與預測

## 快速開始

### 環境需求

- Python 3.8+
- 網路連線（用於 API 資料收集）

### 安裝步驟

```bash
# 1. 複製專案
git clone https://github.com/erik900622/Ubike-ops.git
cd Ubike-ops

# 2. 安裝相依套件
pip install -r requirements.txt

# 3. 啟動資料收集系統
python main.py
```

系統會開始每分鐘收集一次資料，並儲存到本地 SQLite 資料庫（`data/ubike.db`）。

### 啟動儀表板

```bash
# 在另一個終端機執行
streamlit run src/dashboard.py
```

開啟瀏覽器訪問 `http://localhost:8501` 即可查看分析儀表板。

### 產生營運報告

```bash
# 查看當前高風險站點
python scripts/run_analysis.py
```

## 專案成果

### 資料分析成果

- **資料規模**：每分鐘收集 400+ 站點資料，一天累積 57 萬筆資料點
- **分析維度**：時間、空間、站點、區域四個維度交叉分析
- **預測準確度**：基於歷史趨勢的短期預測模型

### 技術亮點

1. **完整的資料管線**：從收集、儲存、清洗、分析到視覺化
2. **可擴展性**：支援本地 SQLite 與雲端 PostgreSQL
3. **程式健壯性**：網路重試機制、錯誤處理、日誌記錄
4. **視覺化設計**：清晰的圖表與互動式介面

## 分析方法說明

### RPI（Rebalancing Priority Index）

用於判斷站點是否需要調度：

```
RPI = 0.5 - (可借車輛 / 總容量)

RPI > 0  → 需要補車（車輛不足）
RPI < 0  → 需要移車（車輛過多）
RPI ≈ 0  → 供需平衡
```

### 趨勢分析

使用「局部斜率平均」方法計算趨勢：

1. 取最近 N 筆資料點
2. 計算每兩筆之間的變化率（bikes/分鐘）
3. 平均所有局部斜率，得到穩定的整體趨勢
4. 根據趨勢預測未來時間點的車輛數

## 資料處理細節

### 資料清洗

- 處理 API 欄位缺失與型別轉換
- 過濾無效座標與異常數值
- 統一時間格式（ISO 8601）

### 資料庫設計

```sql
stations_realtime (
    id               主鍵
    collection_time  收集時間（含索引）
    sno              站點編號
    sna              站點名稱
    sarea            行政區
    lat, lng         座標
    rent             可借車輛數
    return_count     可還車位數
    update_time      API 更新時間
)
```

索引優化：
- `(sno, collection_time)` 複合索引 → 加速單站時序查詢
- `collection_time` 索引 → 加速時間範圍查詢

## 技術棧

- **資料收集**：requests, schedule
- **資料處理**：pandas, sqlalchemy
- **資料儲存**：SQLite (本地) / PostgreSQL (雲端)
- **視覺化**：streamlit, plotly, pydeck
- **其他**：logging (日誌), psycopg2 (PostgreSQL 驅動)

## 未來可延伸方向

1. **進階預測模型**
   - 整合天氣資料（雨天使用量低）
   - 加入週期性分析（工作日 vs 假日）
   - 使用機器學習模型（LSTM, XGBoost）

2. **調度優化演算法**
   - 基於預測結果的自動調度建議
   - 考量運輸成本的最佳化路徑規劃

3. **異常偵測**
   - 識別站點異常行為（如故障、維修）
   - 自動警報系統

## 作者

Erik - Data Analyst

展示技能：
- 資料收集與清洗
- 時間序列分析
- 預測模型建立
- 資料視覺化
- ETL 流程設計

## 授權

MIT License
